# -*- coding: utf-8 -*-
"""computerscience.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xITNkcAzRNPBPcerSyp_RyOgdBeZ3Fb6
"""

import pandas as pd
import random
import json
import numpy as np
from itertools import combinations
import sympy as sy
from sklearn.cluster import AgglomerativeClustering
import re
import matplotlib.colors as mcolors
from collections import defaultdict
import matplotlib.pyplot as plt


random.seed(11)

# Loads the JSON data into a Python dictionary
with open('TVs-all-merged.json') as f:
    data = json.load(f)

# Separates the elements
new_data = {}
i = 1
for key in data.keys():
    for description in data[key]:
        new_data[i] = description
        i+=1

######################### Data Cleaning ###############################

def cleaning_steps(title):
    """
    Takes a title as input and cleans it according to the specified steps
    """
    # Convert title to lowercase
    title = title.lower()
    # Remove characters that are not lowercase letters, numbers, spaces, '.', or '"'
    title = re.sub(r'[^a-z0-9\s\.\"]', '', title)
    # Replace variations of 'inches' and 'hz' with standardised forms ('inch' and 'hz')
    title = re.sub(r' ?inches?| ?inch| ?"| ?hertzs?| ?hz', lambda match: 'inch' if 'inch' in match.group(0) or '"' in match.group(0) else 'hz', title)
    return title

def extract_title_words(data, new_title, regex):
    """
    Extracts the model words, specified by the regex function, from the titles
    """
    for key, title in new_title.items():
        # Use regular expression to find all matches in the title
        extracted = re.findall(regex, title)
        # Flatten the nested list and remove leading/trailing whitespaces
        flattened = [word for group in extracted for word in group if word.strip()]
        new_title[key] = list(set(flattened))

def append_product_color(data, new_title, colorinclusion):
    """
    Appends the color to the list of model words, if that information exists
    """
    if colorinclusion:
        for key, product_data in data.items():
            color_keys = ['Color:', 'Cabinet Color:']  # Keys of interest
            for color_key in color_keys:
                # Only append color if the product contains that key-value pair. Clean in similar fashion to title cleaning.
                if color_key in product_data['featuresMap']:
                    product_color = product_data['featuresMap'][color_key]
                    product_color_cleaned = re.sub("[^a-zA-Z0-9\s\.]", "", product_color.lower())
                    new_title[key] += [product_color_cleaned] # Append color value as a list item


def append_brand_info(data, new_title, includebrandname):
    """
    Appends the brand to the list of model words, if that information exists
    """
    if includebrandname:
        for key, product_data in data.items():
            brand_keys = ['Brand:', 'Brand Name:']  # Keys of interest
            for brand_key in brand_keys:
                # Only append brand if the product contains that key-value pair. Clean in similar fashion to title cleaning.
                if brand_key in product_data['featuresMap']:
                    brand_value = product_data['featuresMap'][brand_key]
                    brand_value_cleaned = re.sub("[^a-zA-Z0-9\s\.]", "", brand_value.lower())
                    new_title[key] += [brand_value_cleaned]  # Append brand value as a list item

def clean_titles(data, includebrandname, includecolors):
    """
    Combines above methods consecutively in order to clean titles, find model words using the prev_regex pattern and append color and brand information
    """
    set_titles = {key: data[key]['title'] for key in data.keys()}
    # Previous regex pattern for finding model words
    prev_regex = r'[a-zA-Z0-9]*[a-zA-Z]+[0-9]+[a-zA-Z0-9]*|[a-zA-Z0-9]*[0-9]+[a-zA-Z]+[a-zA-Z0-9]*|[0-9]+[.][0-9]+[a-zA-Z]*|[0-9]+'

    # All recognized color names from mcolors
    all_colors = '|'.join(mcolors.CSS4_COLORS.keys())

    if includecolors:
      # Combine the regex patterns
      regex = f'({prev_regex})|({all_colors})'
    else:
      regex = prev_regex

    new_title = set_titles.copy()

    # Consecutive execution of above methods
    for key in new_title.keys():
        new_title[key] = cleaning_steps(new_title[key])

    extract_title_words(data, new_title, regex)
    append_product_color(data, new_title, includecolors)
    append_brand_info(data, new_title, includebrandname)
    return new_title

#################### Binary matrix & signature matrix ####################
def create_binary_matrix(data, min_word_frequency):
    """
    Creates the binary vector representation for each product on what descriptive words they contain. Vectors are combined in a matrix.
    """

    # Extract unique words from the dataset
    unique_words = sorted(set(word for value_list in data.values() for word in value_list))

    # Create a binary matrix where values are set to 1 if the word is present in the title
    bin_matrix = np.zeros((len(unique_words), len(data)), dtype=int)

    for col_idx, value_list in enumerate(data.values()):
        for row_idx, word in enumerate(unique_words):
            if word in value_list:
                bin_matrix[row_idx, col_idx] = 1

    # Function used to count the number of times a word occures over all titles
    wordfreq = np.sum(bin_matrix, axis=1)

    # Filter out rows (words) whose sum is not higher than min_word_frequency
    mask = wordfreq >= min_word_frequency
    filtered_binmatrix = bin_matrix[mask]
    filtered_unique_words = np.array(unique_words)[mask]

    # Print dimensions before and after filtering
    print(f"Dimensions before applying word frequency filtering: {bin_matrix.shape}")
    print(f"Dimensions after applying word frequency filtering: {filtered_binmatrix.shape}")

    return filtered_binmatrix

#MinHash algorithm
def generate_hashvalues(lengthbinmatrix, numHashes):
  hash_values = []
  for _ in range(numHashes):
        hash_index = np.random.permutation(lengthbinmatrix) + 1  # Generate random permutations of lengthBinary
        dictionary = {i: hash_index[i] for i in range(lengthbinmatrix)}  # Create a dictionary manually mapping indices to permuted indices
        hash_values.append(dictionary)
  return hash_values

def generate_signatures(binary_matrix, hash_values, num_hashes):
    """
    Returns a list of lists containing signatures for each product (Size: #Products x #numHashes)
    """
    all_signatures = []

    # Number of products being compared
    num_products = binary_matrix.shape[1]

    for product_idx in range(num_products):
        product_signature = []

        for hash_idx in range(num_hashes):
            hash_val = 0  # Initialize hash value

            while hash_val < len(binary_matrix):
                index = hash_values[hash_idx].get(hash_val + 1)  # add 1 to hash value
                if index is not None:
                    if binary_matrix[index - 1, product_idx] == 1:
                        product_signature.append(hash_val + 1)
                        break
                hash_val += 1

        all_signatures.append(product_signature)

    return all_signatures

############################ LSH ############################################
def hash_signatures_to_buckets(products, product_signatures, b):
    """
    Hashes product signatures into buckets for Locality-Sensitive Hashing (LSH).
    """
    n_signatures = len(product_signatures[0])
    if n_signatures % b != 0:
        raise ValueError("The number of signatures cannot be divided by the number of bands!")

    r = int(n_signatures / b)
    product_keys = list(products.keys())  # Get product keys for indexing

    # Initialize bucket_bands as a list of dictionaries
    bucket_bands = [{} for _ in range(b)]

    signature_index = 0

    # Go through all product signatures
    for signature in product_signatures:
        bands = signature_to_bands(signature, r)

        # Go through each band i of a product signature
        for i, band in enumerate(bands.astype(str)):
            band_string = ','.join(band)
            if band_string not in bucket_bands[i]:
                bucket_bands[i][band_string] = []
            bucket_bands[i][band_string].append(product_keys[signature_index])
        signature_index += 1

    return bucket_bands  # Return the list of dictionaries with buckets for each band


def signature_to_bands(signature, rows_per_band):
    """
    Converts a signature into bands for LSH.
    """
    bands = []
    remainder = len(signature) % rows_per_band
    for i in range(0, len(signature) - remainder, rows_per_band):  # Step size: rows_per_band
        bands.append(signature[i:i + rows_per_band])
    # If there's a remainder, pad the last band with zeros
    if remainder > 0:
        last_band = signature[-remainder:] + [0] * (rows_per_band - remainder)
        bands.append(last_band)
    return np.array(bands)

def find_candidate_pairs(bucket_bands):
    """
    Finds candidate pairs from buckets obtained through LSH.
    """
    all_candidates = []
    # Iterate through bands and their corresponding buckets
    for band_idx, band_buckets in enumerate(bucket_bands):
        for bucket_key in band_buckets:
            products_in_bucket = band_buckets[bucket_key]
            if len(products_in_bucket) > 1:
                pairs = generate_pairs(products_in_bucket)
                all_candidates.extend(pairs)

    # Remove duplicates by converting the list to a set and then back to a list
    unique_candidates = list(set(all_candidates))

    return unique_candidates

def generate_pairs(products):
    """
    Generates unique pairs from a list of products.
    """
    num_products = len(products)
    pairs = []
    for i in range(num_products):
        for j in range(i + 1, num_products):
            pairs.append((products[i], products[j]))
    return pairs

#################### MSM ########################
def MSM(candidate_pairs, titles, similarity_treshold, q, w):
    """
    Perform Multi-component Similarity Measure (MSM) to identify final pairs from candidate pairs. A weighted average between Q-grams and Jaccard is calculated.
    """
    final_pairs = []

    def generate_qgrams(titles, q):
        qgrams = []
        for i in range(len(titles) - q + 1):
            qgrams.append(tuple(titles[i:i + q]))  # Convert the list of q-grams into tuples
        return qgrams

    for product1, product2 in candidate_pairs:
        # Calculate q-gram similarity between two titles
        title1_qgrams = generate_qgrams(titles[product1], q)
        title2_qgrams = generate_qgrams(titles[product2], q)

        intersection = len(set(title1_qgrams).intersection(set(title2_qgrams)))
        union = len(set(title1_qgrams).union(set(title2_qgrams)))

        qgram_similarity = intersection / union if union != 0 else 0

        # Calculate Jaccard similarity between two titles
        jaccard_intersection = len(set(titles[product1]).intersection(set(titles[product2])))
        jaccard_union = len(set(titles[product1]).union(set(titles[product2])))

        jaccard_similarity = jaccard_intersection / jaccard_union if jaccard_union != 0 else 0

        # Weighted sum of q-gram and Jaccard similarity
        similarity_score = (w * qgram_similarity) + ((1 - w) * jaccard_similarity)

        if similarity_score > similarity_treshold:
            final_pairs.append((product1, product2))

    return final_pairs


def sample_data(data, n_samples):
    """
    Randomly samples a specified number of items from a dictionary.
    """
    sample_dict = {}
    for i in range(1, n_samples+1):
      draw = random.randint(1, len(data))
      sample_dict[i] = data[draw]
    return sample_dict

def true_positives(candidate_pairs, dataset):
    """
    Calculates the number of true positives from a list of candidate pairs based on modelID.
    """
    TP = 0
    for candidate_pair in candidate_pairs:
      if dataset[candidate_pair[0]]['modelID'] == dataset[candidate_pair[1]]['modelID']:
        TP += 1
    return TP

def total_num_duplicates(dataset, cleaned_titles):
    """
    Identifies and returns all pairs of indices corresponding to duplicate products based on modelID.
    """
    model_ids = {}
    for key, data in dataset.items():
        model_ids[key] = data['modelID']

    model_dict = defaultdict(set)
    for key, model_id in model_ids.items():
        model_dict[model_id].add(key)

    duplicates = [pair for indices in model_dict.values() if len(indices) > 1 for pair in combinations(indices, 2)]

    return duplicates

def sort_sets(candidate_pairs):
    return {(min(pair), max(pair)) for pair in candidate_pairs}

def computeMeasures(duplicates, candidate_pairs, found_pairs):
    """
    Computes evaluation measures (PQ, PC, F1*, F1) for duplicate detection based on candidate pairs and found pairs.
    """
    duplicates = sort_sets(duplicates)
    candidate_pairs = sort_sets(candidate_pairs)
    found_pairs = sort_sets(found_pairs)

    duplicates = set(duplicates)
    total_duplicates = len(duplicates)

    candidate_pairs = set(candidate_pairs)
    found_pairs = set(found_pairs)

    # Measures MSM
    TP = len(found_pairs.intersection(duplicates))
    FP = len(found_pairs) - TP
    FN = total_duplicates - TP

    # Metrics
    PQ = TP / len(candidate_pairs) if len(found_pairs) != 0 else 0
    PC = TP / total_duplicates if total_duplicates != 0 else 0

    if PQ + PC != 0:
        F1_star = (2 * PQ * PC) / (PQ + PC)
    else:
        F1_star = 0

    if TP + FP != 0 and TP + FN != 0:
        precision = TP / (TP + FP)
        recall = TP / (TP + FN)

        if recall + precision != 0:
            F1 = (2 * recall * precision) / (recall + precision)
        else:
            F1 = 0
    else:
        F1 = 0

    return PQ, PC, F1_star, F1

def compute_metrics(data, new_data, bandwidths, bootstraps, n, sim_tres, q, w, min_word_frequency):
    """
    Performs duplicate detection and computes metrics over multiple iterations with bootstrapping.
    """
    iter_count = 0
    hyperparameter_results = {}  # Store results for each hyperparameter pair
    measures = None
    while iter_count != bootstraps:
        # Sample data
        n_samples = int(len(new_data) * 0.63)
        data_subset = sample_data(new_data, n_samples)

        # Create titles
        cleaned_titles = clean_titles(data_subset, includebrandname=True, includecolors=True)

        # Start minhashing
        binary_matrix = create_binary_matrix(cleaned_titles, min_word_frequency)
        hashValues = generate_hashvalues(len(binary_matrix), n)
        signatures = generate_signatures(binary_matrix,hashValues, n)

        duplicates = total_num_duplicates(data_subset, cleaned_titles)

        # Metric values for all band values for the current iteration
        PQ_i = []
        PC_i = []
        F1star_i = []
        F1_i = []
        foc_i = []

        for bandwidth in bandwidths:
            # Apply LSH
            bands = int(bandwidth[0])
            bucket_bands = hash_signatures_to_buckets(cleaned_titles, signatures, bands)
            candidate_pairs = find_candidate_pairs(bucket_bands)

            # Apply MSM
            found_pairs = MSM(candidate_pairs, cleaned_titles, sim_tres, q, w)

            # Compute measures
            PQ, PC, F1_star, F1 = computeMeasures(duplicates, candidate_pairs, found_pairs)
            fraction_of_comparison = len(candidate_pairs) / len(list(combinations(data_subset.keys(), 2)))
            #print("PQ = ", PQ, "PC = ", PC, "F1_star = ", F1_star, "F1 = ", F1)
            PQ_i.append(PQ)
            PC_i.append(PC)
            F1star_i.append(F1_star)
            F1_i.append(F1)
            foc_i.append(fraction_of_comparison)

        if iter_count == 0:
            measures = np.stack((PQ_i, PC_i, F1star_i, F1_i, foc_i), 0)
        else:
            measures += np.stack((PQ_i, PC_i, F1star_i, F1_i, foc_i), 0)

        iter_count += 1

    # Averaged measures with rows=(PQ, PC, F1star, F1) and columns=|settings|
    final_measures = measures / bootstraps
    # Replace the print statements with these lines to generate plots
    plt.figure(figsize=(10, 8))

    plt.subplot(2, 2, 1)
    plt.plot(final_measures[4], final_measures[0])
    plt.xlabel('Fraction of Comparisons')
    plt.ylabel('PQ')
    plt.title('PQ vs Fraction of Comparisons')

    plt.subplot(2, 2, 2)
    plt.plot(final_measures[4], final_measures[1])
    plt.xlabel('Fraction of Comparisons')
    plt.ylabel('PC')
    plt.title('PC vs Fraction of Comparisons')

    plt.subplot(2, 2, 3)
    plt.plot(final_measures[4], final_measures[2])
    plt.xlabel('Fraction of Comparisons')
    plt.ylabel('F1*')
    plt.title('F1* vs Fraction of Comparisons')

    plt.subplot(2, 2, 4)
    plt.plot(final_measures[4], final_measures[3])
    plt.xlabel('Fraction of Comparisons')
    plt.ylabel('F1')
    plt.title('F1 vs Fraction of Comparisons')

    plt.tight_layout()
    plt.show()
    return final_measures

# Initialisation
bootstraps = 5
n = 600
q = 3
min_word_frequency = 1

bandwidths = []

for r in [2, 4, 5, 6, 8, 10, 20, 50, 100, 200]:
    if (n % r) == 0:
        b = n / r
        bandwidths.append([b, r])

# Define lists of values for similarity_threshold and w
similarity_threshold_values = [0.2, 0.4, 0.6]
w_values = [0.3, 0.5, 0.7]
all_results = {}

# Loop over different combinations of similarity_threshold and w
for similarity_threshold in similarity_threshold_values:
    for w in w_values:
        # Call compute_metrics function with varying similarity_threshold and w
        print("Weight used = ", w, "similarity theshold used = ", similarity_threshold)
        result = compute_metrics(data, new_data, bandwidths, bootstraps, n, similarity_threshold, q, w, min_word_frequency)
        all_results[(similarity_threshold, w)] = result

print(all_results)

print(all_results)